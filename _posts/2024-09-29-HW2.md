---
title: DATA 440 HW 2
categories:
- General
feature_image: "https://picsum.photos/2560/600?image=872"
---

**Gradient Boosting with Locally Weighted Regression (LOWESS)**



### **Abstract**

​	This paper evaluates and compares the implementation and evaluation between two machine learning methods: Gradient Boosting with Locally Weighted Regression (Lowess) and Locally Weighted Logistic Regression (LWLR). Gradient Boosting is one of ensemble learning method that trains the model sequentially while each model improves upon the previous model. It is used for improving prediction accuracy. In this study, we combine a Gradient Boosting framework with Lowess to compare it against the eXtreme Gradient Boosting (XGBoost) library. The Locally Weighted Logistic Regression also is compared against the logistic regression for classification tasks using the Iris dataset.



### **1. Introduction**

Gradient Boosting is one of ensemble supervised machine learning algorithm that creates a final, powerful model from multiple weak learners. Although typically decision trees are used as base learners due to their versatility and robustness, this study aims to explore the use of Locally Weighted Regression (Lowess) as a base learner within a Gradient Boosting framework. Lowess captures localized patterns and behaviors in data, allowing non-parametric dataset to be used as in the study.

Locally Weighted Logistic Regression (LWLR) is also implemented to explore its effectiveness in classifications. This will be compared with standard logistic regression to analyze and compare how differences in weighting scheme impacts accuracy in predictions. The study aims to 1) implement a gradient boosting with Lowess and compare its perfomance to XGBoost, 2) explore the effect of different scaling methods on performance of Gradient Boosting and Lowess, and 3) implement and analyze locally weighted logistic regression and standard logistic regression and compare performance. 

### **2. Methodology**

#### **2.1 Gradient Boosting Lowess: Design and Implementation**

The Gradient Boosting Lowess model uses Lowess as the base. Lowess is a non-parametric regression that uses weighted least squares to fit localized linear models to subsets of the data. Because there is no clear trend in the data, this was very suited for this study in capturing non-linear relationships in data. This model trains multiple Lowess models sequentially. Each iteration would allow a new Lowess model to fit on the residual errors from the previous step. This effectively reduces prediction errors.

The number of boosting steps (n_estimators) was allowed to be user-specified, offering flexibility in the depth of boosting. Standard methods such as fit, predict, and is_fitted were included, ensuring compatibility with **Scikit-learn**'s model framework.

To evaluate the effectiveness of **GradientBoostingLowess**, we tested its performance on the **Concrete dataset**, a popular dataset for regression analysis. The dataset was preprocessed using three different scaling methods:

1. **StandardScaler**: This scaler standardizes features by removing the mean and scaling to unit variance.
2. **MinMaxScaler**: This scaler transforms features to a range between a specified minimum and maximum (usually [0, 1]).
3. **QuantileTransformer**: This scaler transforms features based on quantiles to create a normal distribution, which can be effective for handling outliers.

We then conducted **10-fold cross-validation** to ensure robust evaluation and compared the resulting Mean Squared Error (MSE) values to those produced by **XGBoost**, a widely used implementation of gradient boosting known for its efficiency and performance.



#### **2.2 Locally Weighted Logistic Regression: Implementation and Comparison**

The **Locally Weighted Logistic Regression (LWLR)** method was implemented to evaluate its ability to classify data based on local weighting. The primary idea behind LWLR is to fit a logistic regression model locally to each data point, using a weighted Gaussian kernel to emphasize points closer to the target instance.

To assess the efficacy of LWLR, we compared its performance against standard logistic regression using the **Iris dataset**. The **Iris dataset** is a multi-class dataset containing features of different species of Iris flowers. For this study, we converted the classification to a binary problem by distinguishing **Iris-setosa** from other species. The decision boundaries were visualized to highlight the differences in how each model separates the classes.

### **3. Experimental Results**

#### **3.1 Concrete Dataset Analysis: Gradient Boosting Lowess vs. XGBoost**

The **Concrete dataset** was used to evaluate the predictive performance of the GradientBoostingLowess model. To mitigate computational complexity, a subset of **300 samples** was used, and three different scalers were applied to preprocess the features before training.

##### **3.1.1 Mean Squared Error Results**

The Mean Squared Error (MSE) for each configuration of GradientBoostingLowess and XGBoost was as follows:

| **Model**                                    | **MSE Value** |
| -------------------------------------------- | ------------- |
| GradientBoostingLowess (StandardScaler)      | 1.0626e+55    |
| GradientBoostingLowess (MinMaxScaler)        | 1340.71       |
| GradientBoostingLowess (QuantileTransformer) | 5.8846e+121   |
| XGBoost                                      | 24.27         |

##### **3.1.2 Analysis**

The results indicate severe issues with numerical stability for GradientBoostingLowess. Specifically:

- The **StandardScaler** and **QuantileTransformer** produced MSE values of 1.0626e+55 and 5.8846e+121, respectively, which are astronomically high. These results indicate a divergence in the training process, likely caused by numerical instability during the iterative boosting steps. Such issues often occur due to ill-conditioned matrices or when the bandwidth parameter (tau) is not appropriately tuned.
- In contrast, the **MinMaxScaler** produced a more reasonable, albeit still high, MSE of 1340.71. The improvement observed with MinMaxScaler suggests that scaling data to a fixed range reduces the variance and therefore mitigates some instability.
- The **XGBoost** model produced a significantly lower MSE (24.27), demonstrating its superiority in both optimization and numerical stability. XGBoost's in-built regularization and robust optimization techniques are better suited for handling complex regression tasks compared to the implemented GradientBoostingLowess.

##### **3.1.3 Challenges and Limitations**

The inability of GradientBoostingLowess to effectively minimize the error points to several challenges:

- **Numerical Instability**: The nature of the Lowess model, which fits local models based on distance weights, makes it prone to divergence when combined with boosting. The aggregation of local fits in boosting can lead to an explosive model response if the weights are not controlled effectively.
- **Bandwidth Selection**: The choice of bandwidth significantly impacts model performance. Inappropriate bandwidth can cause overfitting or underfitting, which was apparent in the divergence we observed.

#### **3.2 Iris Dataset Analysis: Locally Weighted Logistic Regression vs. Standard Logistic Regression**

The **Iris dataset** was used to evaluate the classification performance of **Locally Weighted Logistic Regression (LWLR)** against standard logistic regression. The dataset was reduced to a binary classification problem and only two features were used to facilitate visualization.

##### **3.2.1 Accuracy Results**

- **Locally Weighted Logistic Regression Accuracy**: 0.76 - 0.80
- **Standard Logistic Regression Accuracy**: 1.00

##### **3.2.2 Decision Boundary Visualization**

Figures 1 and 2 show the decision boundaries generated by **Locally Weighted Logistic Regression** and **Standard Logistic Regression**, respectively.

- The **Locally Weighted Logistic Regression** model generates a complex boundary that attempts to fit localized variations in the data. This model's flexibility can lead to overfitting, particularly when dealing with small datasets or regions with low data density. The resulting decision boundary is more irregular and sensitive to local changes.
- In contrast, the **Standard Logistic Regression** generated a simpler, linear decision boundary that perfectly separates the two classes. This demonstrates the model's effectiveness in finding a global linear decision function that generalizes well to the dataset.

##### **3.2.3 Analysis of Overfitting**

The lower accuracy of **LWLR** compared to standard logistic regression suggests a susceptibility to overfitting. This is further illustrated by the decision boundary, which indicates that LWLR adapts excessively to the local structure of the data, sacrificing generalization.

### **4. Discussion**

The results of this study reveal several important observations regarding the use of **Lowess** and **Locally Weighted Logistic Regression** in machine learning contexts.

1. **Gradient Boosting with Lowess**:
   - The experimental results suggest that **Lowess** is not well-suited as a weak learner within a Gradient Boosting framework without careful adjustment of hyperparameters and appropriate scaling. The significant numerical instability indicates that boosting local models can cause the errors to propagate rather than converge, especially when scaling leads to extreme feature values.
   - **XGBoost**, on the other hand, outperformed GradientBoostingLowess consistently. This is due to XGBoost’s strong regularization and optimization capabilities, which
   - **XGBoost**, on the other hand, outperformed GradientBoostingLowess consistently. This is due to XGBoost’s strong regularization and optimization capabilities, which help to avoid divergence, particularly in situations involving complex relationships and high variance in the features. The incorporation of L1 and L2 regularization in XGBoost likely prevented the overfitting that we saw in GradientBoostingLowess and kept the MSE significantly lower.
   - The instability in **GradientBoostingLowess** could also be attributed to the cumulative nature of boosting when combined with a local regression method like Lowess. In gradient boosting, each subsequent model corrects the residuals of the previous model. Since Lowess focuses on local relationships, the noise in the residuals can be amplified across boosting steps, leading to a divergence in training, especially when features have not been appropriately scaled.

1. **Scaling Effects**:
   - The use of different scaling methods had a significant effect on the performance of GradientBoostingLowess. Specifically:
     - **StandardScaler** and **QuantileTransformer** produced extremely high MSE values. This suggests that these scalers might have generated feature distributions with higher variances or outliers, leading to numerical instabilities in the local weight calculations.
     - The **MinMaxScaler** performed comparatively better, likely due to the consistent range of [0, 1] reducing the impact of outlier weights during the local regression fitting.
2. **Locally Weighted Logistic Regression**:
   - The **Locally Weighted Logistic Regression** (LWLR) was found to be less effective than **standard logistic regression** on the Iris dataset, with accuracy values ranging between **0.76 to 0.80**.
   - The **decision boundary** generated by LWLR, as depicted in Figure 1, was more irregular and sensitive to individual data points. While this localized adaptability can be beneficial in non-linear scenarios, in this context it resulted in overfitting, especially due to the small sample size.
   - On the other hand, **standard logistic regression** achieved perfect accuracy, suggesting that a simpler, global decision function was more suitable for this dataset. The boundary in Figure 2 was linear, consistent, and effectively separated the two classes.

### **5. Conclusion**

The study presented in this paper explored the application of **Gradient Boosting** with **Lowess** and the use of **Locally Weighted Logistic Regression**. The primary conclusions are:

1. **Challenges with Gradient Boosting Lowess**:
   - The use of **Lowess** as a weak learner in Gradient Boosting is fraught with challenges, including severe numerical instability and the propagation of local errors across boosting iterations.
   - The experiment demonstrated that without careful scaling and hyperparameter optimization, Gradient Boosting Lowess could produce extremely high error values, rendering the model unsuitable for practical use. In contrast, **XGBoost** showed stable and accurate predictions, highlighting the advantages of using decision trees combined with strong regularization and optimized gradient boosting algorithms.
2. **Impact of Scaling**:
   - Scaling is critical in determining the performance of Lowess. The **MinMaxScaler** offered a more stable transformation, resulting in a reasonable error metric, whereas **StandardScaler** and **QuantileTransformer** led to divergence in the model. The study indicates that the feature range should be controlled to prevent large weight variations when using locally weighted methods.
3. **Locally Weighted Logistic Regression**:
   - **Locally Weighted Logistic Regression** showed limited effectiveness in the classification task, as its localized fitting led to overfitting. The irregular decision boundary formed by LWLR highlighted its sensitivity to local variations, which may not be ideal for datasets like **Iris** with globally separable classes. **Standard logistic regression**, with its simplicity and consistency, produced perfect classification accuracy, demonstrating its superiority for this task.

### **6. Future Work**

The findings of this study suggest multiple areas for future work:

1. **Hyperparameter Optimization**:
   - The performance of GradientBoostingLowess can be improved by more effectively tuning the hyperparameters, particularly the **bandwidth (****tau****)** and **number of boosting steps (****n_estimators****)**. Automated hyperparameter optimization methods such as **Grid Search** or **Bayesian Optimization** could be used to identify the best values.
2. **Regularization**:
   - Introducing **regularization** within the Lowess model fitting may prevent overfitting and improve numerical stability. Regularization could help control the influence of local weights, particularly in regions with high variance.
3. **Ensemble of Lowess**:
   - Instead of sequential boosting, combining multiple Lowess models using an ensemble averaging approach could help reduce individual model errors while avoiding the propagation of errors inherent in boosting.
4. **Application on Larger Datasets**:
   - Extending this study to **larger datasets** would provide more insights into the scalability of GradientBoostingLowess. A larger dataset could help understand whether the boosting mechanism can generalize well when more data points are available for localized fitting.
5. **Comparison with Other Non-Parametric Methods**:
   - Future studies could compare **GradientBoostingLowess** with other **non-parametric methods**, such as **k-Nearest Neighbors** in an ensemble framework, to further understand the capabilities and limitations of using local models in boosting.