---
title: Waste Vehicle Optimization
categories:
- General
feature_image: "https://picsum.photos/2560/600?image=872"
---
***DATA 440 Final**

--------------------

## Index

- Imbalanced datasets
- The metric trap
- Confusion matrix
- Resampling
- Random under-sampling
- Random over-sampling
- Python imbalanced-learn module
- Random under-sampling and over-sampling with imbalanced-learn
- Under-sampling: Tomek links
- Under-sampling: Cluster Centroids
- Over-sampling: SMOTE
- Over-sampling followed by under-sampling
- Recommended reading

## Imbalanced datasets

In this kernel we will know some techniques to handle highly unbalanced datasets, with a focus on resampling. The Porto Seguro's Safe Driver Prediction competition, used in this kernel, is a classic problem of unbalanced classes, since insurance claims can be considered unusual cases when considering all clients. Other classic examples of unbalanced classes are the detection of financial fraud and attacks on computer networks.

Let's see how unbalanced the dataset is:



```python
train_data = pd.read_csv('train.csv')

data_0 = train_data[train_data['target'] == 0].sample(frac=0.1, random_state=42)
data_1 = train_data[train_data['target'] == 1].sample(frac=0.1, random_state=42)
subset_data = pd.concat([data_0, data_1])

X = subset_data.drop(columns=['target', 'id'])  
y = subset_data['target']

target_count = y.value_counts()

print('Class 0:', target_count[0])
print('Class 1:', target_count[1])
print('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')

colors = ['blue', 'orange']  
target_count.plot(kind='bar', color=colors, title='Count (target)')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()
```

Class 0: 57352 <br>Class 1: 2169 <br>Proportion: 26.44 : 1<br>

![bar1]({{ site.baseurl }}/assets/images/bar1.png)



SMOTE

```python
def apply_smote(X_train, y_train):
    smote = SMOTE(random_state=42)
    X_res, y_res = smote.fit_resample(X_train, y_train)
    return X_res, y_res
```



ADASYN and KDE

```python
def apply_adasyn_kde(X_train, y_train):
    adasyn = ADASYN(random_state=42)
    X_res, y_res = adasyn.fit_resample(X_train, y_train)
    
    # KDE to smooth the distribution of generated samples
    kde = gaussian_kde(X_res.T)
    X_res_kde = kde.resample(len(X_res)).T
    return X_res_kde, y_res
```



```python
class SimpleFlow(nn.Module):
    def __init__(self, input_dim):
        super(SimpleFlow, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, input_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

def apply_normalizing_flows(X_train, y_train):
    # Filter only minority class samples
    minority_class = X_train[y_train == 1]
    minority_tensor = torch.tensor(minority_class.values, dtype=torch.float32)
    
    flow = SimpleFlow(input_dim=minority_tensor.shape[1])
    optimizer = optim.Adam(flow.parameters(), lr=0.001)
    
    # Train flow model (placeholder example, minimal training for demonstration)
    for epoch in range(10):  # Typically requires more epochs
        optimizer.zero_grad()
        output = flow(minority_tensor)
        loss = ((output - minority_tensor)**2).mean()
        loss.backward()
        optimizer.step()
    
    # Generate synthetic samples
    with torch.no_grad():
        synthetic_samples = flow(minority_tensor).numpy()
    
    X_res = np.vstack([X_train.values, synthetic_samples])
    y_res = np.concatenate([y_train.values, np.ones(len(synthetic_samples))])
    return X_res, y_res
```

```python
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

results = []
start_time = time.time()

for train_index, test_index in skf.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    for method in ['SMOTE', 'ADASYN-KDE', 'Normalizing Flows']:
        if method == 'SMOTE':
            X_res, y_res = apply_smote(X_train, y_train)
        elif method == 'ADASYN-KDE':
            X_res, y_res = apply_adasyn_kde(X_train, y_train)
        elif method == 'Normalizing Flows':
            X_res, y_res = apply_normalizing_flows(X_train, y_train)

        X_res = pd.DataFrame(X_res, columns=X_train.columns)

        clf = RandomForestClassifier(n_estimators=50, random_state=42)
        clf.fit(X_res, y_res)
        
        y_pred = clf.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        rec = recall_score(y_test, y_pred)
        cm = confusion_matrix(y_test, y_pred)

        results.append({
            'Method': method,
            'Accuracy': acc,
            'Recall': rec,
            'Confusion Matrix': cm
        })

end_time = time.time()
print(f"Total runtime: {end_time - start_time:.2f} seconds")

results_df = pd.DataFrame(results)
print(results_df[['Method', 'Accuracy', 'Recall']])
```

Total runtime: 713.80 seconds <br>              Method  Accuracy    Recall 0               SMOTE  0.962369  0.000000 1          ADASYN-KDE  0.565225  0.442396 2   Normalizing Flows  0.963545  0.000000 3               SMOTE  0.962366  0.004619 4          ADASYN-KDE  0.491935  0.494226 5   Normalizing Flows  0.963626  0.000000 6               SMOTE  0.962282  0.006912 7          ADASYN-KDE  0.568800  0.394009 8   Normalizing Flows  0.963542  0.000000 9               SMOTE  0.961610  0.002304 10         ADASYN-KDE  0.591482  0.384793 11  Normalizing Flows  0.963542  0.000000 12              SMOTE  0.961610  0.000000 13         ADASYN-KDE  0.589214  0.453917 14  Normalizing Flows  0.963542  0.000000

```python
def analyze_class_distribution(y_res, method_name):
    print(f"\nClass distribution after applying {method_name}:")
    print(pd.Series(y_res).value_counts(normalize=True))

for method in ['SMOTE', 'ADASYN-KDE', 'Normalizing Flows']:
    analyze_class_distribution(y_res, method)
```

 Class distribution after applying SMOTE: <br>0.0    0.929689 1.0    0.070311 <br>Name: proportion, dtype: float64 <br>Class distribution after applying ADASYN-KDE: <br>0.0    0.929689 1.0    0.070311 <br>Name: proportion, dtype: float64 <br>Class distribution after applying Normalizing Flows: <br>0.0    0.929689 1.0    0.070311 <br>Name: proportion, dtype: float64<br>

```python
class SimpleAutoencoder(nn.Module):
    def __init__(self, input_dim):
        super(SimpleAutoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(32, 64),
            nn.ReLU(),
            nn.Linear(64, input_dim)
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

def apply_normalizing_flows(X_train, y_train):
    minority_class = X_train[y_train == 1]
    minority_tensor = torch.tensor(minority_class.values, dtype=torch.float32)
    
    autoencoder = SimpleAutoencoder(input_dim=minority_tensor.shape[1])
    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)
    
    for epoch in range(20):  
        optimizer.zero_grad()
        output = autoencoder(minority_tensor)
        loss = ((output - minority_tensor) ** 2).mean()
        loss.backward()
        optimizer.step()
    
    with torch.no_grad():
        synthetic_samples = autoencoder(minority_tensor).numpy()
    
    X_res = np.vstack([X_train.values, synthetic_samples])
    y_res = np.concatenate([y_train.values, np.ones(len(synthetic_samples))])
    return pd.DataFrame(X_res, columns=X_train.columns), y_res
```

```python
results_df = pd.DataFrame(results)
print(results_df[['Method', 'Accuracy', 'Recall']])

for method in results_df['Method'].unique():
    cm_list = [result['Confusion Matrix'] for result in results if result['Method'] == method]
    cm = np.round(np.mean(cm_list, axis=0)).astype(int)  
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues') 
    plt.title(f'Confusion Matrix for {method}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()
```

â€‹               Method  Accuracy    Recall 0               SMOTE  0.962369  0.000000 1          ADASYN-KDE  0.565225  0.442396 2   Normalizing Flows  0.963545  0.000000 3               SMOTE  0.962366  0.004619 4          ADASYN-KDE  0.491935  0.494226 5   Normalizing Flows  0.963626  0.000000 6               SMOTE  0.962282  0.006912 7          ADASYN-KDE  0.568800  0.394009 8   Normalizing Flows  0.963542  0.000000 9               SMOTE  0.961610  0.002304 10         ADASYN-KDE  0.591482  0.384793 11  Normalizing Flows  0.963542  0.000000 12              SMOTE  0.961610  0.000000 13         ADASYN-KDE  0.589214  0.453917 14  Normalizing Flows  0.963542  0.000000



![cm1]({{ site.baseurl }}/assets/images/cm1.png)

![cm2]({{ site.baseurl }}/assets/images/cm2.png)

![cm3]({{ site.baseurl }}/assets/images/cm3.png)

Summary of Results: <br>Method: SMOTE <br>Average Accuracy: 0.9620 <br>Average Recall: 0.0028 <br>Note: SMOTE has very low recall, indicating poor performance in detecting the minority class. <br>Method: ADASYN-KDE <br>Average Accuracy: 0.5613 <br>Average Recall: 0.4339 <br>Note: ADASYN-KDE shows moderate recall, suggesting it is reasonably effective in addressing class imbalance. <br>Method: Normalizing Flows <br>Average Accuracy: 0.9636 <br>Average Recall: 0.0000 <br>Note: Normalizing Flows has very low recall, indicating poor performance in detecting the minority class.<br>
