---
title: Smoothly Clipped Absolute Deviation (SCAD) penalty 
categories:
- General
feature_image: "https://picsum.photos/2560/600?image=872"
---
**DATA 440 HW 3**

--------------------

Smoothly Clipped Absolute Deviation (SCAD) is a non-concave penalty function [Fan and Li (2001)](https://fan.princeton.edu/sites/g/files/toruqf5476/files/documents/penlike.pdf). It aims to solve variable selection bias problem that comes from other methods such as LASSO regression while retaining sparcity.

<!-- The function goes as follows:

$$p_{\lambda}^{SCAD}(\beta_j) = 
\begin{cases} 
  \lambda |\beta_j| & \text{if } |\beta_j| \leq \lambda, \\
  \frac{a\lambda|\beta_j| - 0.5(\beta_j^2 + \lambda^2)}{a-1} & \text{if } \lambda < |\beta_j| \leq a\lambda, \\
  \frac{(a+1)\lambda^2}{2} & \text{if } |\beta_j| > a\lambda.
\end{cases}$$ -->

In this assignment, ElasticNet and SqrtLasso will further be used to compare their efficiency in different areas, as they are all popular for handling high-dimensional data. ElasticNet, a combination of Lasso and Ridge, manages correlated features and does variable selection. SqrtLasso is a improvement of Lasso by incorporating the square root of the residual sum of squares, effectively reducing noise. 

PyTorch will be further be used and the models will be applied on both simulated and real datasets. Their performance will be measured with R2 score and mean squared error (MSE).

For more references on SCAD, please visit:

1. https://andrewcharlesjones.github.io/journal/scad.html
2. https://www.jstor.org/stable/27640214?seq=1
3. https://www.mayo.edu/research/documents/scad-documentationpdf/DOC-10026887
4. https://fan.princeton.edu/sites/g/files/toruqf5476/files/documents/penlike.pdf



Import necessary library

```python
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.linalg import toeplitz

from sklearn.metrics import r2_score
from sklearn.linear_model import ElasticNet as SklearnElasticNet
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.model_selection import KFold, train_test_split, GridSearchCV
```

```python
class SCADRegression(nn.Module):
    def __init__(self, input_dim, a=3.7, lambda_=0.01, device="cpu"):
        super(SCADRegression, self).__init__()
        # Initialize with small random values instead of zeros
        self.beta = nn.Parameter(torch.randn(input_dim, dtype=torch.float64, device=device) * 0.01)
        self.a = a
        self.lambda_ = lambda_

    def scad_penalty(self, beta):
        lambda_ = self.lambda_
        a = self.a
        penalty = torch.where(torch.abs(beta) <= lambda_,
                              lambda_ * torch.abs(beta),
                              torch.where(torch.abs(beta) <= a * lambda_,
                                          (a * lambda_ * torch.abs(beta) - 0.5 * (beta ** 2 + lambda_ ** 2)) / (a - 1),
                                          0.5 * (a + 1) * lambda_ ** 2))
        return penalty.sum()

    def forward(self, X, y):
        predictions = torch.matmul(X, self.beta)
        loss = torch.mean((y - predictions) ** 2) + self.scad_penalty(self.beta)
        return loss

    def fit(self, X, y, num_epochs=1000, lr=0.01):
        optimizer = torch.optim.Adam([self.beta], lr=lr)
        for epoch in range(num_epochs):
            optimizer.zero_grad()
            loss = self.forward(X, y)
            loss.backward()
            optimizer.step()
        if (epoch + 1) % 100 == 0:
                    print(f"Epoch {epoch+1}, Loss: {loss.item()}")
                    print(f"Updated Coefficients: {self.beta.detach().cpu().numpy()}")

    def predict(self, X):
        return torch.matmul(X, self.beta).detach()

    def get_coefficients(self):
        return self.beta.detach().cpu().numpy()
```

```python
class ElasticNet(nn.Module):
    def __init__(self, input_size, alpha=1.0, l1_ratio=0.5):
        super(ElasticNet, self).__init__()
        self.linear = nn.Linear(input_size, 1, bias=False).double()
        self.alpha = alpha
        self.l1_ratio = l1_ratio

    def forward(self, X):
        return self.linear(X)

    def loss(self, y_pred, y_true):
        mse_loss = nn.MSELoss()(y_pred, y_true)
        l1_reg = torch.norm(self.linear.weight, p=1)
        l2_reg = torch.norm(self.linear.weight, p=2)
        return mse_loss + self.alpha * (self.l1_ratio * l1_reg + (1 - self.l1_ratio) * l2_reg ** 2)

    def fit(self, X, y, num_epochs=1000, lr=0.01):
        optimizer = torch.optim.Adam(self.parameters(), lr=lr)
        for epoch in range(num_epochs):
            optimizer.zero_grad()
            y_pred = self.forward(X)
            loss = self.loss(y_pred, y)
            loss.backward()
            optimizer.step()
            if (epoch + 1) % 100 == 0:
                print(f"Epoch {epoch+1}, Loss: {loss.item()}")

    def predict(self, X):
        return self.forward(X).detach()

    def get_coefficients(self):
        return self.linear.weight.detach().cpu().numpy()
```

```python
class SqrtLasso(nn.Module):
    def __init__(self, input_size, alpha=0.1):
        super(SqrtLasso, self).__init__()
        self.linear = nn.Linear(input_size, 1, bias=False).double()
        self.alpha = alpha

    def forward(self, X):
        return self.linear(X)

    def loss(self, y_pred, y_true):
        mse_loss = nn.MSELoss()(y_pred, y_true)
        l1_reg = torch.norm(self.linear.weight, p=1)
        return torch.sqrt(mse_loss) + self.alpha * l1_reg

    def fit(self, X, y, num_epochs=1000, lr=0.01):
        optimizer = torch.optim.Adam(self.parameters(), lr=lr)
        for epoch in range(num_epochs):
            optimizer.zero_grad()
            y_pred = self.forward(X)
            loss = self.loss(y_pred, y)
            loss.backward()
            optimizer.step()
            if (epoch + 1) % 100 == 0:
                print(f"Epoch {epoch+1}, Loss: {loss.item()}")

    def predict(self, X):
        return self.forward(X).detach()

    def get_coefficients(self):
        return self.linear.weight.detach().cpu().numpy()
```



```python
def cross_validation_scad(X_train, y_train, lambdas, num_folds=5, num_epochs=1000, lr=0.05):
    best_lambda = None
    best_mse = float('inf')

    for lambda_ in lambdas:
        scad_model = SCADRegression(input_dim=X_train.shape[1], lambda_=lambda_)
        kf = KFold(n_splits=num_folds)
        fold_mse = []

        for train_idx, val_idx in kf.split(X_train):
            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]
            y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]

            scad_model.fit(X_fold_train, y_fold_train, num_epochs=num_epochs, lr=lr)
            val_predictions = scad_model.predict(X_fold_val)
            mse = nn.MSELoss()(val_predictions, y_fold_val).item()
            fold_mse.append(mse)

        avg_mse = np.mean(fold_mse)
        if avg_mse < best_mse:
            best_mse = avg_mse
            best_lambda = lambda_

    print(f"Best Lambda for SCAD: {best_lambda}, MSE: {best_mse}")
    return SCADRegression(input_dim=X_train.shape[1], lambda_=best_lambda)

# Extend the lambda search space and increase learning rate
lambdas = [0.0001, 0.001, 0.01, 0.1, 1, 10, 50, 100]
scad_concrete = cross_validation_scad(X_train_tensor, y_train_tensor, lambdas)
```

Best Lambda for SCAD: 100, MSE: 1569.8834545138045



Now we evaluate feature importance based on the SCAD coefficients from the model.

```python
def analyze_feature_importance(scad_model, feature_names):
    coefficients = scad_model.get_coefficients()
    selected_indices = np.where(coefficients != 0)[0]

    if len(selected_indices) == 0:
        print("No features selected by SCAD.")
        return []

    selected_features = [feature_names[i] for i in selected_indices]
    important_coeffs = coefficients[selected_indices]

    # Sort by absolute value of coefficients
    importance = sorted(zip(selected_features, important_coeffs), key=lambda x: abs(x[1]), reverse=True)
    print("Selected Features and Their Importance (in descending order):")
    for feature, coeff in importance:
        print(f"{feature}: {coeff}")

    return importance
```

analyze_feature_importance function identifies which features have non-zero coefficients which are already selected by SCAD regularization and sorts them by importance.
We can interpret the results of the SCAD and see which variables are important.  



```python
def plot_feature_importance(importance):
    if not importance:
        print("No features to plot.")
        return

    features, coefficients = zip(*importance)
    plt.figure(figsize=(10, 6))
    plt.barh(features, coefficients, color="skyblue")
    plt.xlabel("Coefficient Value")
    plt.title("Feature Importance Based on SCAD Coefficients")
    plt.gca().invert_yaxis()
    plt.show()

feature_names = poly.get_feature_names_out(input_features=data.columns.drop('strength'))
feature_importance = analyze_feature_importance(scad_concrete, feature_names)

plot_feature_importance(feature_importance)
```
![FeatureImportanceBasedonSCADCoefficients]({{ site.baseurl }}/assets/images/Feature.png)

There are many features that influence the outcome. Many of them effect it positively (bar to the right), such as cement age, superplastic age, and water.
There are some features that impact outcome negatively such as slag water. A few are not important of close to 0. By looking at the result, we can conclude which features are matter to most in handling concrete. 


```python
def cross_validation_sqrtlasso(X_train, y_train, alphas, num_folds=5, num_epochs=1000, lr=0.01):
    best_alpha = None
    best_mse = float('inf')

    for alpha in alphas:
        sqrt_lasso_model = SqrtLasso(input_size=X_train.shape[1], alpha=alpha)
        kf = KFold(n_splits=num_folds)
        fold_mse = []

        for train_idx, val_idx in kf.split(X_train):
            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]
            y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]

            sqrt_lasso_model.fit(X_fold_train, y_fold_train, num_epochs=num_epochs, lr=lr)
            val_predictions = sqrt_lasso_model.predict(X_fold_val)
            mse = nn.MSELoss()(val_predictions, y_fold_val).item()
            fold_mse.append(mse)

        avg_mse = np.mean(fold_mse)
        if avg_mse < best_mse:
            best_mse = avg_mse
            best_alpha = alpha

    print(f"Best Alpha for SqrtLasso: {best_alpha}, MSE: {best_mse}")
    return SqrtLasso(input_size=X_train.shape[1], alpha=best_alpha)

# Cross-validate SqrtLasso
alphas = [0.001, 0.01, 0.1, 1, 10]
sqrt_lasso_concrete = cross_validation_sqrtlasso(X_train_tensor, y_train_tensor, alphas)
```

Best Alpha for SqrtLasso: 0.01<br>MSE: 1414.251638621219

```python
# Load the dataset - use concrete.csv
data = pd.read_csv("https://github.com/dvasiliu/AAML/blob/main/Data%20Sets/concrete.csv?raw=true")

# Prepare feature matrix (X) and target (y)
X = data.drop(columns=["strength"]).values
y = data["strength"].values

# Polynomial features (degree 2)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)

# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Using PyTorch tensors
X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float64)
y_train_tensor = torch.tensor(y_train, dtype=torch.float64).reshape(-1, 1)
X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float64)
y_test_tensor = torch.tensor(y_test, dtype=torch.float64).reshape(-1, 1)

# Cross validation for SCAD
lambdas = [0.001, 0.01, 0.1, 1, 10]
def cross_validation_scad(X_train, y_train, lambdas, num_folds=5, num_epochs=1000, lr=0.01):
    best_lambda = None
    best_mse = float('inf')

    for lambda_ in lambdas:
        scad_model = SCADRegression(input_dim=X_train.shape[1], lambda_=lambda_)
        kf = KFold(n_splits=num_folds)
        fold_mse = []

        for train_idx, val_idx in kf.split(X_train):
            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]
            y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]

            scad_model.fit(X_fold_train, y_fold_train, num_epochs=num_epochs, lr=lr)
            val_predictions = scad_model.predict(X_fold_val)
            mse = nn.MSELoss()(val_predictions, y_fold_val).item()
            fold_mse.append(mse)

        avg_mse = np.mean(fold_mse)
        if avg_mse < best_mse:
            best_mse = avg_mse
            best_lambda = lambda_

    print(f"Best Lambda for SCAD: {best_lambda}, MSE: {best_mse}")
    return SCADRegression(input_dim=X_train.shape[1], lambda_=best_lambda)

scad_concrete = cross_validation_scad(X_train_tensor, y_train_tensor, lambdas)
feature_names = poly.get_feature_names_out(input_features=data.columns.drop("strength"))
feature_importance = analyze_feature_importance(scad_concrete, feature_names)
plot_feature_importance(feature_importance)

# Cross-Validation for SqrtLasso
alphas = [0.001, 0.01, 0.1, 1, 10]
def cross_validation_sqrtlasso(X_train, y_train, alphas, num_folds=5, num_epochs=1000, lr=0.01):
    best_alpha = None
    best_mse = float('inf')

    for alpha in alphas:
        sqrt_lasso_model = SqrtLasso(input_size=X_train.shape[1], alpha=alpha)
        kf = KFold(n_splits=num_folds)
        fold_mse = []

        for train_idx, val_idx in kf.split(X_train):
            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]
            y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]

            sqrt_lasso_model.fit(X_fold_train, y_fold_train, num_epochs=num_epochs, lr=lr)
            val_predictions = sqrt_lasso_model.predict(X_fold_val)
            mse = nn.MSELoss()(val_predictions, y_fold_val).item()
            fold_mse.append(mse)

        avg_mse = np.mean(fold_mse)
        if avg_mse < best_mse:
            best_mse = avg_mse
            best_alpha = alpha

    print(f"Best Alpha for SqrtLasso: {best_alpha}, MSE: {best_mse}")
    return SqrtLasso(input_size=X_train.shape[1], alpha=best_alpha)

sqrt_lasso_concrete = cross_validation_sqrtlasso(X_train_tensor, y_train_tensor, alphas)

# Cross-validation for ElasticNet
param_grid = {
    'alpha': [0.001, 0.01, 0.1, 1, 10],
    'l1_ratio': [0.2, 0.5, 0.7, 1.0]
}
elastic_net_cv = GridSearchCV(SklearnElasticNet(max_iter=10000), param_grid, cv=5)
elastic_net_cv.fit(X_train_scaled, y_train)

print(f"Best ElasticNet Parameters: {elastic_net_cv.best_params_}")

# Predictions on test data
scad_test_pred = scad_concrete.predict(X_test_tensor).reshape(-1, 1)
en_test_pred = elastic_net_cv.predict(X_test_scaled).reshape(-1, 1)  # Using the best ElasticNet model from CV
sl_test_pred = sqrt_lasso_concrete.predict(X_test_tensor).reshape(-1, 1)

# MSE for each model
mse_scad = nn.MSELoss()(scad_test_pred, y_test_tensor).item()
mse_en = nn.MSELoss()(torch.tensor(en_test_pred), y_test_tensor).item()
mse_sl = nn.MSELoss()(sl_test_pred, y_test_tensor).item()

print(f"SCAD Test MSE: {mse_scad}")
print(f"ElasticNet Test MSE: {mse_en}")
print(f"SqrtLasso Test MSE: {mse_sl}")

# Function to compute and return R² score
def evaluate_r2_score(y_true, y_pred):
    return r2_score(y_true, y_pred)

# R² for each model
r2_scad = evaluate_r2_score(y_test_tensor.numpy().flatten(), scad_test_pred.numpy().flatten())
r2_elasticnet = evaluate_r2_score(y_test_tensor.numpy().flatten(), en_test_pred.flatten())
r2_sqrtlasso = evaluate_r2_score(y_test_tensor.numpy().flatten(), sl_test_pred.numpy().flatten())

print("\nR² Score Comparison")
print(f"SCAD R² Score:        {r2_scad:.4f}")
print(f"ElasticNet R² Score:  {r2_elasticnet:.4f}")
print(f"SqrtLasso R² Score:   {r2_sqrtlasso:.4f}")

# R² scores in a bar chart
r2_scores = [r2_scad, r2_elasticnet, r2_sqrtlasso]
model_names = ['SCAD', 'ElasticNet', 'SqrtLasso']

plt.figure(figsize=(8, 5))
plt.bar(model_names, r2_scores, color=['blue', 'green', 'orange'])
plt.xlabel('Models')
plt.ylabel('R² Score')
plt.title('R² Score Comparison Across Models')
plt.show()
```
![R2Score]({{ site.baseurl }}/assets/images/R2Score.png)

SCAD model has R2 score around -4.5. It has a poor performance.
ElasticNet model is close to 0, slightly positive yet still does not show great performance.
SqrtLasso model is around -4 and does not do well in predicting. 
Out of three, ElasticNet performs the best. 
The reason for this may be because this was performed on the real data and the hyperparameters were not explored well. 

```python
def make_correlated_features(num_samples, p, rho=0.9):
    vcor = [rho ** i for i in range(p)]
    r = toeplitz(vcor)
    mu = np.zeros(p)
    x = np.random.multivariate_normal(mu, r, size=num_samples)
    return x

X_sim = make_correlated_features(200, 20, rho=0.9)
beta_true = np.array([1.5, -2, 3] + [0] * 17)
y_sim = X_sim @ beta_true + np.random.randn(200) * 0.1  # Adding noise
X_sim = torch.tensor(X_sim, dtype=torch.float64)
y_sim = torch.tensor(y_sim, dtype=torch.float64).reshape(-1, 1)

# R² and coefficient comparison
def compare_coefficients(true_beta, estimated_beta):
    plt.plot(true_beta, label='True Beta')
    plt.plot(estimated_beta, label='Estimated Beta', linestyle='dashed')
    plt.legend()
    plt.xlabel("Coefficient Index")
    plt.ylabel("Value")
    plt.title("Coefficient Comparison")
    plt.show()

# R² score
def evaluate_r2_score(model, X, y_true):
    y_pred = model.predict(X)
    return r2_score(y_true, y_pred)

# Fit models (SCAD, ElasticNet, SqrtLasso) and compare
scad_model = SCADRegression(input_dim=X_sim.shape[1])
elastic_net_model = ElasticNet(input_size=X_sim.shape[1], alpha=0.01, l1_ratio=0.5)
sqrt_lasso_model = SqrtLasso(input_size=X_sim.shape[1], alpha=0.1)

# Train models
scad_model.fit(X_sim, y_sim, num_epochs=1000)
elastic_net_model.fit(X_sim, y_sim, num_epochs=1000)
sqrt_lasso_model.fit(X_sim, y_sim, num_epochs=1000)

# Extract coefficients
scad_coeffs = scad_model.get_coefficients()
elastic_net_coeffs = elastic_net_model.get_coefficients().flatten()
sqrt_lasso_coeffs = sqrt_lasso_model.get_coefficients().flatten()

# Compare coefficients
compare_coefficients(beta_true, scad_coeffs)
compare_coefficients(beta_true, elastic_net_coeffs)
compare_coefficients(beta_true, sqrt_lasso_coeffs)

# Evaluate R² scores
r2_scad = evaluate_r2_score(scad_model, X_sim, y_sim.numpy().flatten())
r2_en = evaluate_r2_score(elastic_net_model, X_sim, y_sim.numpy().flatten())
r2_sl = evaluate_r2_score(sqrt_lasso_model, X_sim, y_sim.numpy().flatten())

print(f"R² SCAD: {r2_scad}")
print(f"R² ElasticNet: {r2_en}")
print(f"R² SqrtLasso: {r2_sl}")
```
![coefficient1]({{ site.baseurl }}/assets/images/coefficient1.png)
R2 for SCAD is -0.0016, showing that this performed poorly. 
![coefficient2]({{ site.baseurl }}/assets/images/coefficient2.png)
R2 for ElasticNet is 0.9966, which means this performed extremely well.
![coefficient3]({{ site.baseurl }}/assets/images/coefficient3.png)
R2 for SqrtLasso was 0.9975, and this performed even better than ElasticNet. 
It seems like ElasticNet and SqrtLasso are very effective in capturing these kind of data. 

R² SCAD: -0.0016019338370691916 <br>R² ElasticNet: 0.9966055989636963 <br>R² SqrtLasso: 0.9975080250229935

```python
# Model Testing on Simulated Data:
# Instantiate the models
scad_model = SCADRegression(input_dim=20)
elastic_net_model = ElasticNet(input_size=20, alpha=0.01, l1_ratio=0.5)
sqrt_lasso_model = SqrtLasso(input_size=20, alpha=0.1)

# Fit models
scad_model.fit(X_sim, y_sim, num_epochs=1000, lr=0.01)
elastic_net_model.fit(X_sim, y_sim, num_epochs=1000, lr=0.01)
sqrt_lasso_model.fit(X_sim, y_sim, num_epochs=1000, lr=0.01)

# Get coefficients
scad_coeffs = scad_model.get_coefficients()
elastic_net_coeffs = elastic_net_model.get_coefficients()
sqrt_lasso_coeffs = sqrt_lasso_model.get_coefficients()

# Display recovered coefficients
print("SCAD Coefficients:", scad_coeffs)
print("ElasticNet Coefficients:", elastic_net_coeffs)
print("SqrtLasso Coefficients:", sqrt_lasso_coeffs)
```

SCAD Coefficients: [ 2.45916168e-03  1.85347900e-03 -3.59173345e-03 -6.04682384e-04 -6.59646528e-04  7.93157710e-04 -7.49979641e-04 -2.94007673e-03  1.07287430e-03  1.33466781e-03 -9.46859899e-04 -3.29887543e-05 -2.77498508e-04 -5.84661320e-04  1.01204747e-03 -1.34239949e-03 -2.51593613e-04  6.25854166e-04 -3.17518449e-03  1.58764215e-03] <br>ElasticNet Coefficients: [[ 1.35389039e+00 -1.60259898e+00  2.55595019e+00  2.37184074e-01  -9.46587857e-02  2.63863333e-02 -1.17797258e-02  9.14454197e-03   1.55244149e-05 -1.85317917e-05  5.15759223e-02 -2.12430900e-02  -2.21067454e-02 -9.03497633e-05  3.44286496e-02 -1.02714118e-04  -4.24956335e-03  7.91087542e-05 -2.39551627e-02 -1.69121212e-02]] <br>SqrtLasso Coefficients: [[ 1.41108394e+00 -1.82309053e+00  2.89791304e+00 -1.13297565e-03   6.10349581e-04 -1.40416827e-04  1.71157831e-05 -6.99102342e-04   2.26917526e-03 -2.66756442e-04 -1.95184244e-04  9.79365517e-04  -1.58837599e-03  8.24952334e-04  3.48673416e-04  1.19658394e-03   1.71500240e-03 -1.56332052e-03 -4.59104510e-04 -4.27808550e-04]]



-----

**Test and Evaluate the models**

```python
def betastar_recovery(true_beta, estimated_beta):
    mse_coefficients = np.mean((true_beta - estimated_beta) ** 2)
    print(f"Coefficient MSE: {mse_coefficients}")

    plt.figure(figsize=(10, 6))
    plt.plot(true_beta, label='True Beta (Betastar)', color='blue')
    plt.plot(estimated_beta, label='Estimated Beta', color='red', linestyle='dashed')
    plt.xlabel("Coefficient Index")
    plt.ylabel("Coefficient Value")
    plt.title("Betastar vs Estimated Coefficients")
    plt.legend()
    plt.show()

    return mse_coefficients

betastar = np.array([1.5, -2, 3] + [0] * 17)

scad_coefficients = scad_model.get_coefficients()
elasticnet_coefficients = elastic_net_model.get_coefficients().flatten()
sqrtlasso_coefficients = sqrt_lasso_model.get_coefficients().flatten()

# Betastar recovery MSE
scad_mse = betastar_recovery(betastar, scad_coefficients)
elasticnet_mse = betastar_recovery(betastar, elasticnet_coefficients)
sqrtlasso_mse = betastar_recovery(betastar, sqrtlasso_coefficients)
```

![betastar1]({{ site.baseurl }}/assets/images/betastar1.png)
Coefficient MSE: 0.7635819991995422<br>
This is a high error. It did a poor job in recovering the true coefficients. SCAD did not capture the sparcity pattern of betas well. 

![betastar2]({{ site.baseurl }}/assets/images/betastar2.png)
Coefficient MSE: 0.02241271788600953<br>
This is a moderate error. ElasticNet is better at SCAD yet it still can improve. 

![betastar3]({{ site.baseurl }}/assets/images/betastar3.png)
Coefficient MSE: 0.0024821846656431596<br>
SqrtLasso did very well in capturing the true betas. 
<br>
In conclusion, SCAD did not perform well, most likely its need for more tuning and sensitivity to the complexity of the real datasets.<br>
ElasticNet performed reasonably well with R2 score close to 1 in both simulated and real datasets, which makes it a good choices when data has correlated features. <br>
SqrtLasso performs best. It had the smallest coefficient MSE and the highest R2 score in both datasets, which showed its capability to capture sparse signals and reduce noise. <br>

Further experiments with SCAD is necessary as it is a good variable selection method yet depending on datasets ElasticNet and SqrtLasso may be more reliable. 